{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1201300617.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install langchain langchain-community faiss-cpu sentence-transformers octoai-sdk langchain-text-splitters lxml tiktoken python-dotenv 'arize-phoenix[evals]'\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# OctoAI\n",
    "# ! pip install langchain langchain-community faiss-cpu sentence-transformers octoai-sdk langchain-text-splitters lxml tiktoken python-dotenv 'arize-phoenix[evals]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OCTOAI_API_TOKEN = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IjNkMjMzOTQ5In0.eyJzdWIiOiJkNTY1Y2Q3YS0zYmNjLTQzNDgtOGQxYy1mMGY0ZjY0ODkyYzciLCJ0eXBlIjoidXNlckFjY2Vzc1Rva2VuIiwidGVuYW50SWQiOiJkMzRmOGVkZC1kMzE1LTQ4NTktOTc0Zi03MjJiMzNlNDA5ZDIiLCJ1c2VySWQiOiJhZTMxM2ExZS1lYTI3LTRkOTItYjk5OS1iOTNlY2Q0YjQ3NTgiLCJhcHBsaWNhdGlvbklkIjoiYTkyNmZlYmQtMjFlYS00ODdiLTg1ZjUtMzQ5NDA5N2VjODMzIiwicm9sZXMiOlsiRkVUQ0gtUk9MRVMtQlktQVBJIl0sInBlcm1pc3Npb25zIjpbIkZFVENILVBFUk1JU1NJT05TLUJZLUFQSSJdLCJhdWQiOiIzZDIzMzk0OS1hMmZiLTRhYjAtYjdlYy00NmY2MjU1YzUxMGUiLCJpc3MiOiJodHRwczovL2lkZW50aXR5Lm9jdG8uYWkiLCJpYXQiOjE3MTk2Nzg3NTl9.m3RpRr3vLoGekWhvk1fXHlTBAg1fkkO2V2eMfXddjLf30taFXnAOLoJIWHsGLupliLVW3JL9_wxPRhL2cGpDH_1_PeWDUYxKk_ZY1RjL9s_yoJf7fGeEy9-VPbmvjfYXcQTa_sxE99GeqwaODud26e_O04Q9n2KiNSwdeeion6ki9ctpIbDXZ3Wit_ltumGY0axDRInfm_QHwvo92r4W-3g8jN4lskS7ZA7CT5LCRq7GzFs5E2HsZq-59RnVhxnray6nG5fyG3U67Ff0ZCTuRaDY2DqRDOgQw1zRqTYMoW8zeOfozRxlGygdRcRU3AxnUOGOi7yijetkE9I60H9qlg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 676, which is longer than the specified 512\n",
      "Created a chunk of size 626, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"../workout_data\")\n",
    "file_texts = []\n",
    "for file in files:\n",
    "    with open(f\"../workout_data/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512, chunk_overlap=64, \n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text, \n",
    "                metadata={\"doc_title\": file.split(\".\")[0], \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! model is not default parameter.\n",
      "                model was transferred to model_kwargs.\n",
      "                Please confirm that model is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\n",
    "llm = OctoAIEndpoint(\n",
    "        model=\"meta-llama-3-8b-instruct\",\n",
    "        max_tokens=1024,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        octoai_api_token=OCTOAI_API_TOKEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are a data analyst. You will be given unstructured data texts and figure out the schema. Return the schema. Be as general as possible so the schema can be used on most data input. Return the schema as PosgreSQL CREATE TABLE query.\n",
    "Document: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Here is the schema for the given documents in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"page_content\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"doc_title\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"chunk_num\": {\n",
      "          \"type\": \"integer\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This schema represents a JSON object with two properties: `page_content` and `metadata`. `page_content` is a string, and `metadata` is an object with two properties: `doc_title` and `chunk_num`, both of which are strings and integers respectively. \n",
      "\n",
      "This schema can be applied to all the given documents as they have the same structure. The `page_content` property contains the text content of the document, and the `metadata` property contains additional information about the document, such as its title and chunk number. \n",
      "\n",
      "Note that this schema is quite general and does not capture the specific structure of the text content, which appears to be a workout routine. If you want to capture the structure of the text content, you would need a more specific schema that takes into account the specific format of the workout routine. For example, you might want to define a schema for a workout routine that includes properties for the exercise, sets, reps, and weight. However, the schema I provided above is a good starting point for capturing the basic structure of the documents. \n",
      "\n",
      "Also, please note that the schema I provided is not a perfect fit for the given documents. For example, the `page_content` property is a string, but it contains multiple paragraphs and sections, which could be represented as separate properties in a more specific schema. Similarly, the `metadata` property is an object, but it contains only two properties, which could be represented as separate properties in a more specific schema. However, the schema I provided above is a good starting point for capturing the basic structure of the documents. \n",
      "\n",
      "If you want to capture the specific structure of the text content, you would need to analyze the text content more closely and define a more specific schema that takes into account the specific format of the workout routine. For example, you might want to define a schema for a workout routine that includes properties for the exercise, sets, reps, and weight, as well as properties for the day of the workout and the muscle group being targeted. \n",
      "\n",
      "Here is an example of a more specific schema that captures the structure of the text content:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"workout_routine\": {\n",
      "      \"type\": \"array\",\n",
      "      \"items\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"day\": {\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"muscle_group\": {\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          \"exercises\": {\n",
      "            \"type\": \"array\",\n",
      "            \"items\": {\n",
      "              \"type\": \"object\",\n",
      "              \"properties\": {\n",
      "                \"exercise\": {\n",
      "                  \"type\": \"string\"\n",
      "                },\n",
      "                \"sets\": {\n",
      "                  \"type\": \"integer\"\n",
      "                },\n",
      "                \"reps\": {\n",
      "                  \"type\": \"integer\"\n",
      "                },\n",
      "                \"weight\": {\n",
      "                  \"type\": \"string\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This schema represents a JSON object with a single property `workout_routine`, which is an array of objects. Each object in the array represents a day of the workout routine and has three properties: `day`, `muscle_group`, and `exercises`. The `day` property is a string, the `muscle_group` property is a string, and the `exercises` property is an array of objects. Each object in the `exercises` array represents an exercise and has three properties: `exercise`, `sets`, `reps`, and `weight`. The `exercise` property is a string, the `sets` and `reps` properties are integers, and the `weight` property is a string. \n",
      "\n",
      "This schema is more specific than the original schema and captures the specific structure of the text content. However, it is still a simplification and may not capture all the nuances of the text content. For example, it does not capture the specific format of the exercise names or the units of measurement for the weight. \n",
      "\n",
      "In general, defining a schema for unstructured text content can be a challenging task, and it may require a combination of natural language processing techniques and domain-specific knowledge. \n",
      "\n",
      "I hope this helps! Let me know if you have any further questions. \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "\n",
      "Data Analyst \n",
      "[Your Company] \n",
      "[Your Email] \n",
      "[Your Phone Number] \n",
      "\n",
      "Please note that the schema I provided is a general schema and may not capture all the nuances of the text content. If you\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Show me the schema as a json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
